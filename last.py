# -*- coding: utf-8 -*-
"""Last

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLCi4yoicfK0YBHwAJ5g2WXNmCX7EzEy
"""

import streamlit as st
import numpy as np
import pandas as pd
import yfinance as yf
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Function to fetch data from Yahoo Finance
def fetch_data(tickers, start, end):
    all_data = {}
    invalid_tickers = []
    earliest_common_date = pd.Timestamp(start)

    for ticker in tickers:
        try:
            data = yf.download(ticker, start=start, end=end)
            if data.empty:
                invalid_tickers.append(ticker)
            else:
                all_data[ticker] = data['Adj Close']
                if data.index[0] > earliest_common_date:
                    earliest_common_date = data.index[0]
        except Exception as e:
            st.error(f"Error fetching data for {ticker}: {e}")
            invalid_tickers.append(ticker)

    if not all_data:
        return pd.DataFrame(), invalid_tickers

    # Filter all data to start from the earliest common date
    for ticker in all_data:
        all_data[ticker] = all_data[ticker][all_data[ticker].index >= earliest_common_date]

    # Combine all data into a single DataFrame
    combined_data = pd.DataFrame(all_data)

    return combined_data, invalid_tickers

# Function to train and evaluate models
def train_and_evaluate_models(X_train, X_test, y_train, y_test):
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'LSTM': Sequential([
            LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),
            LSTM(50, return_sequences=False),
            Dense(25),
            Dense(y_train.shape[1])
        ])
    }

    rmse_scores = {}

    # Linear Regression and Random Forest
    for name, model in models.items():
        if name != 'LSTM':
            model.fit(X_train.reshape(X_train.shape[0], -1), y_train)
            y_pred = model.predict(X_test.reshape(X_test.shape[0], -1))
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            rmse_scores[name] = rmse

    # LSTM Model
    X_train_lstm = X_train
    X_test_lstm = X_test
    models['LSTM'].compile(optimizer='adam', loss='mean_squared_error')
    models['LSTM'].fit(X_train_lstm, y_train, epochs=5, batch_size=1, verbose=1)
    y_pred_lstm = models['LSTM'].predict(X_test_lstm)
    rmse_scores['LSTM'] = np.sqrt(mean_squared_error(y_test, y_pred_lstm))

    best_model_name = min(rmse_scores, key=rmse_scores.get)
    best_model = models[best_model_name]

    return best_model, best_model_name, rmse_scores

# Function to simulate portfolios and calculate metrics
def simulate_portfolios(mean_returns, cov_matrix, num_portfolios=10000, risk_free_rate=0.01):
    results = np.zeros((3, num_portfolios))
    weights_record = []

    for i in range(num_portfolios):
        weights = np.random.random(len(mean_returns))
        weights /= np.sum(weights)

        portfolio_return = np.sum(weights * mean_returns) * 252
        portfolio_std_dev = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)
        sharpe_ratio = (portfolio_return - risk_free_rate) / portfolio_std_dev

        results[0, i] = portfolio_return
        results[1, i] = portfolio_std_dev
        results[2, i] = sharpe_ratio
        weights_record.append(weights)

    return results, weights_record

# Streamlit main function
def main():
    st.title("Stock Analysis and Portfolio Optimization")

    tickers = st.text_input("Enter stock tickers separated by commas (e.g., AAPL, MSFT, GOOGL):").split(',')
    tickers = [ticker.strip() for ticker in tickers]

    if st.button('Fetch Data'):
        data, unavailable_tickers = fetch_data(tickers, start="2000-01-01", end="2024-06-10")

        if unavailable_tickers:
            st.warning(f"The ticker(s) {', '.join(unavailable_tickers)} not available for analysis. Please enter appropriate ticker(s).")
            st.stop()
        #elif data is None or data.empty:
            #st.warning("No data fetched for the given tickers. Please enter appropriate ticker(s).")
            #st.stop()

        st.write("Head of the Data")
        st.write(data.head())
        st.write("Tail of the Data")
        st.write(data.tail())

        st.subheader("Historical Stock Prices")
        st.line_chart(data)

        # Calculate daily returns
        returns = data.pct_change().dropna()

        if returns.empty:
            st.warning("No returns data available. Please enter valid tickers.")
            st.stop()

        st.subheader("Summary Statistics")
        st.write(returns.describe())

        st.subheader("Return Distributions")
        fig, ax = plt.subplots(figsize=(10, 7))
        returns.plot(kind='density', ax=ax)
        ax.set_title('Return Distributions')
        ax.set_xlabel('Return')
        st.pyplot(fig)

        st.subheader("Correlation Matrix")
        correlation_matrix = returns.corr()
        st.write(correlation_matrix)

        fig, ax = plt.subplots(figsize=(10, 7))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, ax=ax)
        ax.set_title('Correlation Matrix Heatmap')
        st.pyplot(fig)

        # Number of portfolios to simulate
        num_portfolios = 10000

        # Extract the mean returns and the covariance matrix of the returns
        mean_returns = returns.mean()
        cov_matrix = returns.cov()

        # Risk-free rate for Sharpe ratio calculation (e.g., 1% annual risk-free rate)
        risk_free_rate = 0.01

        # Simulate portfolios
        results, weights_record = simulate_portfolios(mean_returns, cov_matrix, num_portfolios, risk_free_rate)

        # Convert results array to Pandas DataFrame
        results_frame = pd.DataFrame(results.T, columns=['Return', 'Standard Deviation', 'Sharpe Ratio'])

        # Locate the portfolio with the maximum Sharpe ratio
        max_sharpe_idx = results_frame['Sharpe Ratio'].idxmax()
        max_sharpe_port = results_frame.iloc[max_sharpe_idx]
        max_sharpe_weights = weights_record[max_sharpe_idx]

        # Locate the portfolio with the minimum standard deviation
        min_vol_idx = results_frame['Standard Deviation'].idxmin()
        min_vol_port = results_frame.iloc[min_vol_idx]
        min_vol_weights = weights_record[min_vol_idx]

        st.header("Historical Maximum Sharpe Ratio Portfolio Allocation")
        st.subheader("Annualized Return")
        st.write(f"{max_sharpe_port[0]}")
        st.subheader("Annualized Standard Deviation")
        st.write(f"{max_sharpe_port[1]}")
        st.subheader("Sharpe Ratio")
        st.write(f"{max_sharpe_port[2]}")
       #st.subheader("Weights")
        #st.write(f"{max_sharpe_weights}")

        st.header("Historical Minimum Volatility Portfolio Allocation")
        st.subheader("Annualized Return")
        st.write(f"{min_vol_port[0]}")
        st.subheader("Annualized Standard Deviation")
        st.write(f"{min_vol_port[1]}")
        st.subheader("Sharpe Ratio")
        st.write(f"{min_vol_port[2]}")
        #st.subheader("Weights")
        #st.write(f"{min_vol_weights}")

        # Prepare data for forecasting
        X = returns[:-1].values
        y = returns.shift(-1).dropna().values

        # Reshape X and y for LSTM
        X = X.reshape(X.shape[0], X.shape[1], 1)
        y = y.reshape(y.shape[0], y.shape[1])

        # Train-test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train and evaluate models
        best_model, best_model_name, rmse_scores = train_and_evaluate_models(X_train, X_test, y_train, y_test)
        st.write(f"Best model: {best_model_name}")
        st.write(f"RMSE Scores: {rmse_scores}")

        # Make predictions using the best model
        if best_model_name == 'LSTM':
            predicted_returns = pd.Series(best_model.predict(X_test)[-1], index=tickers)
        else:
            predicted_returns = pd.Series(best_model.predict(X_test.reshape(X_test.shape[0], -1))[-1], index=tickers)

        # Simulate portfolios based on predicted returns
        mean_returns_pred = predicted_returns  # Use predicted returns directly
        cov_matrix_pred = returns.cov()
        results_pred, weights_record_pred = simulate_portfolios(mean_returns_pred, cov_matrix_pred)

        # Convert results array to Pandas DataFrame
        results_frame_pred = pd.DataFrame(results_pred.T, columns=['Return', 'Standard Deviation', 'Sharpe Ratio'])

        # Locate the portfolio with the maximum Sharpe ratio
        max_sharpe_idx_pred = results_frame_pred['Sharpe Ratio'].idxmax()
        max_sharpe_port_pred = results_frame_pred.iloc[max_sharpe_idx_pred]
        max_sharpe_weights_pred = weights_record_pred[max_sharpe_idx_pred]

        # Locate the portfolio with the minimum standard deviation
        min_vol_idx_pred = results_frame_pred['Standard Deviation'].idxmin()
        min_vol_port_pred = results_frame_pred.iloc[min_vol_idx_pred]
        min_vol_weights_pred = weights_record_pred[min_vol_idx_pred]

        st.header("Predicted Maximum Sharpe Ratio Portfolio Allocation")
        st.subheader("Annualized Return")
        st.write(f"{max_sharpe_port_pred[0]}")
        st.subheader("Annualized Standard Deviation")
        st.write(f"{max_sharpe_port_pred[1]}")
        st.subheader("Sharpe Ratio")
        st.write(f"{max_sharpe_port_pred[2]}")
        #st.subheader("Weights")
        #st.write(f"{max_sharpe_weights_pred}")

        st.header("Predicted Minimum Volatility Portfolio Allocation")
        st.subheader("Annualized Return")
        st.write(f"{min_vol_port_pred[0]}")
        st.subheader("Annualized Standard Deviation")
        st.write(f"{min_vol_port_pred[1]}")
        st.subheader("Sharpe Ratio")
        st.write(f"{min_vol_port_pred[2]}")
        #st.subheader("Weights")
        #st.write(f"{min_vol_weights_pred}")

        # Display weights for historical and predicted minimum volatility portfolio
        min_vol_weights_df = pd.DataFrame({
            'Historical': min_vol_weights,
            'Predicted': min_vol_weights_pred
        }, index=tickers)
        st.write("\nWeights for Minimum Volatility Portfolio:\n")
        st.write(min_vol_weights_df)

        # Display weights for historical and predicted maximum Sharpe ratio portfolio
        max_sharpe_weights_df = pd.DataFrame({
            'Historical': max_sharpe_weights,
            'Predicted': max_sharpe_weights_pred
        }, index=tickers)
        st.write("\nWeights for Maximum Sharpe Ratio Portfolio:\n")
        st.write(max_sharpe_weights_df)

        # Plot the historical efficient frontier
        fig, ax = plt.subplots(figsize=(10, 7))
        scatter = ax.scatter(results_frame['Standard Deviation'], results_frame['Return'], c=results_frame['Sharpe Ratio'], cmap='viridis')
        colorbar = plt.colorbar(scatter, ax=ax)
        colorbar.set_label('Sharpe Ratio')
        ax.scatter(max_sharpe_port[1], max_sharpe_port[0], color='r', marker='*', s=200, label='Max Sharpe Ratio')
        ax.scatter(min_vol_port[1], min_vol_port[0], color='b', marker='*', s=200, label='Min Volatility')
        ax.set_title('Historical Efficient Frontier')
        ax.set_xlabel('Annualized Standard Deviation')
        ax.set_ylabel('Annualized Return')
        ax.legend()
        st.pyplot(fig)

        # Plot the predicted efficient frontier
        fig, ax = plt.subplots(figsize=(10, 7))
        scatter = ax.scatter(results_frame_pred['Standard Deviation'], results_frame_pred['Return'], c=results_frame_pred['Sharpe Ratio'], cmap='viridis')
        colorbar = plt.colorbar(scatter, ax=ax)
        colorbar.set_label('Sharpe Ratio')
        ax.scatter(max_sharpe_port_pred[1], max_sharpe_port_pred[0], color='r', marker='*', s=200, label='Max Sharpe Ratio')
        ax.scatter(min_vol_port_pred[1], min_vol_port_pred[0], color='b', marker='*', s=200, label='Min Volatility')
        ax.set_title('Predicted Efficient Frontier')
        ax.set_xlabel('Annualized Standard Deviation')
        ax.set_ylabel('Annualized Return')
        ax.legend()
        st.pyplot(fig)

        # Plot the weights of the historical and predicted minimum volatility portfolio as pie charts
        fig, axs = plt.subplots(1, 2, figsize=(14, 7))
        min_vol_weights_df['Historical'].plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=axs[0], legend=False)
        axs[0].set_title('Historical Minimum Volatility Portfolio Weights')
        axs[0].set_ylabel('')
        min_vol_weights_df['Predicted'].plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=axs[1], legend=False)
        axs[1].set_title('Predicted Minimum Volatility Portfolio Weights')
        axs[1].set_ylabel('')
        st.pyplot(fig)

        # Plot the weights of the historical and predicted maximum Sharpe ratio portfolio as pie charts
        fig, axs = plt.subplots(1, 2, figsize=(14, 7))
        max_sharpe_weights_df['Historical'].plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=axs[0], legend=False)
        axs[0].set_title('Historical Maximum Sharpe Ratio Portfolio Weights')
        axs[0].set_ylabel('')
        max_sharpe_weights_df['Predicted'].plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=axs[1], legend=False)
        axs[1].set_title('Predicted Maximum Sharpe Ratio Portfolio Weights')
        axs[1].set_ylabel('')
        st.pyplot(fig)

# Run the main function
if __name__ == "__main__":
    main()